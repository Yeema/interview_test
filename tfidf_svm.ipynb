{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=open('Ch_trainfile_Sentiment_3000.txt','r',encoding='utf-8').readlines()\n",
    "labels=[]\n",
    "strings=[]\n",
    "for kk in k:\n",
    "    segments=kk.split()\n",
    "    try:\n",
    "        int(segments[0])\n",
    "        labels.append(segments[0])\n",
    "        str1=''.join(segments[1:len(segments)])\n",
    "        strings.append(str1)\n",
    "    except ValueError:\n",
    "        str1=segments[0]\n",
    "        strings[-1]=strings[-1]+str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "import jieba\n",
    "import codecs\n",
    "\n",
    "\"\"\"\n",
    "1. 文本切割\n",
    "\"\"\"\n",
    "# sentence='这样的酒店配这样的价格还算不错'\n",
    "def sent2word(sentence):\n",
    "    \"\"\"\n",
    "    Segment a sentence to words\n",
    "    Delete stopwords\n",
    "    \"\"\"\n",
    "    segList = jieba.cut(sentence)\n",
    "    segResult = []\n",
    "    for w in segList:\n",
    "        segResult.append(w)\n",
    "\n",
    "    fd = open('stopword.txt','r',encoding='utf-8').readlines()\n",
    "    newSent = []\n",
    "    stopwords=[s[:-1] for s in fd]\n",
    "    for word in segResult:\n",
    "        if word in stopwords:\n",
    "#             print (\"stopword: %s\" % word)\n",
    "            continue\n",
    "        else:\n",
    "            newSent.append(word)\n",
    "    if not newSent:\n",
    "        newSent=segResult\n",
    "    return newSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/jm/88rqwfqs02l30zlfmtxtc9g80000gn/T/jieba.cache\n",
      "Loading model cost 16.891 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "texts=[]\n",
    "for i in strings:\n",
    "    texts.append(' '.join(sent2word(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2987, 2666)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(texts)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "930"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get('商场')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2987, 12421)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_count_vect = CountVectorizer(ngram_range=(1, 5))\n",
    "XX_train_counts = ngram_count_vect.fit_transform(texts)\n",
    "XX_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2987, 2666)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(texts[:int(len(texts)*0.8)], labels[:int(len(texts)*0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try one\n",
    "predicted = text_clf.predict(texts[int(len(texts)*0.8):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7023411371237458\n"
     ]
    }
   ],
   "source": [
    "# try one\n",
    "correct=0\n",
    "for id,label in enumerate(labels[int(len(texts)*0.8):]):\n",
    "    if label==predicted[id]:\n",
    "        correct+=1\n",
    "print('accuracy:',(correct/(id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 3 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(text_clf, texts, labels, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_training_id(p):\n",
    "    ids=[]\n",
    "    for i in range(len(texts)):\n",
    "        rand=random.uniform(0, 1)\n",
    "        if rand in p:\n",
    "            ids.append(p.index(rand))\n",
    "        else:\n",
    "            ids.append(bisect.bisect_left(p, rand))\n",
    "    return ids\n",
    "def update_possibility(p,pred_labels,tested_labels):\n",
    "    wrong=[]\n",
    "    correct=[]\n",
    "    # test 90 examples by classifier trained by 10 examples\n",
    "    for id,x in enumerate(zip(tested_labels,pred_labels)):\n",
    "        if x[0]==x[1]:\n",
    "            correct.append(id)\n",
    "        else:\n",
    "            wrong.append(id)\n",
    "    # record the possibility of the wrongly classified examples \n",
    "    error=[p[e] for e in wrong]\n",
    "    # count total error rate\n",
    "    error=sum(error)\n",
    "    # calculate beta\n",
    "    beta=math.sqrt( abs(error/(1-error)))\n",
    "    # correct examples will be multiplied by beta\n",
    "    # incorrect examples will be divided by beta\n",
    "    if error>1e-10:\n",
    "        for e in wrong:\n",
    "            p[e]=p[e]/beta\n",
    "        for c in correct:\n",
    "            p[e]=p[e]*beta\n",
    "    # normalize sum of the possibility to be 1\n",
    "    total=sum(p)\n",
    "    p=[x/total for x in p]\n",
    "    sum_p=[]\n",
    "    # accumulate the normalized possibility\n",
    "    for i in range(len(p)):\n",
    "        sum_p.append(sum(p[:i+1]))\n",
    "    return p,sum_p\n",
    "def cal_result(testset,gt):\n",
    "    # use 9 classifiers to estimate examples' type\n",
    "    # if more than half of classifiers agree with 'some type'\n",
    "    # then that is the final result\n",
    "    acc=[]\n",
    "    sum_result=0\n",
    "    # print(len(testset),len(testset[0]),len(gt),len(gt[0])) 3 300 300 3\n",
    "    for i in range(num_trial):\n",
    "        temp=[]\n",
    "        for x in zip(testset[i],gt):\n",
    "            # print(x)\n",
    "            if x[0]==x[1]:\n",
    "                temp.append(1)\n",
    "            else:\n",
    "                temp.append(0)\n",
    "        acc.append(temp)\n",
    "    # print('acc:',len(acc),len(acc[0]),len(testset),len(testset[0]))\n",
    "    for i in range(len(testset[0])):#300\n",
    "        temp=0\n",
    "        for j in range(len(acc)):#3\n",
    "            temp+=acc[j][i]\n",
    "        # print(temp,acc[0][j],acc[1][j],acc[2][j],len(acc))\n",
    "        if temp>int(len(acc)/2):\n",
    "            sum_result+=1\n",
    "    return float(sum_result/len(gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
